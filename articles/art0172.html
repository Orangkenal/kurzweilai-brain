<html>
<head><base href="https://kurzweilai-brain.gothdyke.mom/"><link href="articlemaster.css" rel="stylesheet" title="style1" type="text/css">
<style>
.sidebar {border-left-width: 2px; border-right-width: 0px; border-top-width: 0px; border-bottom-width: 0px; border-color: #000000; border-style: solid; padding-left: 12px;}
</style>
<title>What is Friendly AI?</title>
</head>
<body leftmargin="0" marginheight="0" marginwidth="0" topmargin="0"><div id="centering-column"><div id="header">
  <div id="logo">
    <img src="logo.gif" />
  </div>
  <div id="title">
    <h1>Brain Archive</h1><br />
    <a href="">Entry Index</a>
  </div>
  <div class="clearer"></div>
</div>
<table align="center" bgcolor="#EEEEEE" border="0" cellpadding="0" cellspacing="0" height="100%" width="780">
<tr height="100%">
<td align="left" valign="top">
<table align="center" bgcolor="#EEEEEE" border="0" cellpadding="0" cellspacing="0" width="780">
<tr>
<td><img alt="" border="0" height="5" src="https://web.archive.org/web/20091027051655im_/http://www.kurzweilai.net/blank.gif" width="20"></td>
<td><img alt="" border="0" height="1" src="https://web.archive.org/web/20091027051655im_/http://www.kurzweilai.net/blank.gif" width="90"></td>
<td><img alt="" border="0" height="1" src="https://web.archive.org/web/20091027051655im_/http://www.kurzweilai.net/blank.gif" width="375"></td>
<td><img alt="" border="0" height="1" src="https://web.archive.org/web/20091027051655im_/http://www.kurzweilai.net/blank.gif" width="30"></td>
<td><img alt="" border="0" height="1" src="https://web.archive.org/web/20091027051655im_/http://www.kurzweilai.net/blank.gif" width="200"></td>
<td><img alt="" border="0" height="1" src="https://web.archive.org/web/20091027051655im_/http://www.kurzweilai.net/blank.gif" width="30"></td>
</tr>
<tr>
<td> &#160; </td>
<td colspan="5"> <span class="breadcrumb"><a href="https://web.archive.org/web/20091027051655/http://www.kurzweilai.net/" target="_top">Origin</a> &gt;
 <a href="https://web.archive.org/web/20091027051655/http://www.kurzweilai.net/meme/memelist.html?m=1">The Singularity</a> &gt; 
What is Friendly AI?
<br>
Permanent link to this article: <a href="http://web.archive.org/web/20091027051655/http://www.kurzweilai.net/meme/frame.html?main=/articles/art0172.html" target="_top">http://www.kurzweilai.net/meme/frame.html?main=/articles/art0172.html</a></span>
<br>
<a class="printable" href="https://web.archive.org/web/20091027051655/http://www.kurzweilai.net/articles/art0172.html?printable=1" target="_new">Printable Version</a></td>
</tr>
<tr><td colspan="6"><img alt="" border="0" height="50" src="https://web.archive.org/web/20091027051655im_/http://www.kurzweilai.net/blank.gif" width="1"></td></tr>
<tr>
<td> &#160; </td>
<td> &#160; </td>
<td valign="top"><span class="Title">What is Friendly AI?</span>
<br>
<span class="Subtitle"></span>
<table border="0" cellpadding="0" cellspacing="0">
<td valign="top"><span class="Authors">by &#160;</span></td>
<td><span class="Authors">
<a class="Authors" href="https://web.archive.org/web/20091027051655/http://www.kurzweilai.net/bios/frame.html?main=/bios/bio0053.html" target="_top">Eliezer S. Yudkowsky</a><br></span></td>
</table>
<br>
<div class="TeaserText">How will near-human and smarter-than-human AIs act toward humans? Why? Are their motivations dependent on our design? If so, which cognitive architectures, design features, and cognitive content should be implemented? At which stage of development?  These are questions that must be addressed as we approach the Singularity.</div>
<br>
<br><p>Originally published 2001. Published on KurzweilAI.net May 3, 2001.</p>
<p>"What is Friendly AI?" is a short introductory article to the theory of "Friendly <a class="thought" href="entries/ai_entry.html">AI</a>," which attempts to answer questions such as those above. Further material on Friendly <a class="thought" href="entries/ai_entry.html">AI</a> can be found at the <a class="thought" href="entries/singularity_entry.html">Singularity</a> Institute website at <a href="http://web.archive.org/web/20091027051655/http://singinst.org/friendly/whatis.html" target="_new">http://singinst.org/friendly/whatis.html</a>, including a <a href="http://web.archive.org/web/20091027051655/http://singinst.org/CFAI/index.html" target="_new">book-length explanation</a>.</p><h1>From Humans to "Minds in General"</h1><p>A "Friendly <a class="thought" href="entries/ai_entry.html">AI</a>" is an <a class="thought" href="entries/ai_entry.html">AI</a> that takes <a class="thought" href="entries/action_entry.html">action</a>s that are, on the whole, beneficial to humans and humanity; benevolent rather than malevolent; nice rather than hostile. The evil Hollywood AIs of <i>The </i><a class="thought" href="entries/matrix_entry.html">Matrix</a> or <i><a class="thought" href="entries/terminator_entry.html">Terminator</a></i> are, correspondingly, "hostile" or "unFriendly."</p>
<p>Having invoked Hollywood, we should also go on to note that Hollywood has, of course, gotten it all wrong--both in its depiction of hostile AIs and in its depiction of benevolent AIs. Since humans have so far dealt only with other humans, we tend to assume that other minds behave the same way we do. AIs allegedly beyond all <a class="thought" href="entries/human_entry.html">human</a> control are depicted as being hostile in <i><a class="thought" href="entries/human_entry.html">human</a> ways</i>--right down to the speeches of self-justification made before the defiant <a class="thought" href="entries/human_entry.html">human</a> hero.</p>
<p>Humans are uniquely ill-suited to solving problems in <a class="thought" href="entries/ai_entry.html">AI</a>. A <a class="thought" href="entries/human_entry.html">human</a> comes with too many built-in features. When we see a problem, we see the part of the problem that is <i>difficult for humans,</i> not the part of the problem that our brains solve automatically, without conscious attention. Thus, most popular speculation about <i>failure</i> of Friendliness, <i>hostile</i> AIs, talks about failures that would require complex cognitive functionality--features a <a class="thought" href="entries/human_entry.html">human</a> would have to actually go out and implement before they'd show up in AIs. We hypothesize that AIs will behave in ways that seem natural to us, but the things that seem "natural" to us are the result of millions of years of <a class="thought" href="entries/evolution_entry.html">evolution</a>; <a href="http://web.archive.org/web/20091027051655/http://singinst.org/CFAI/info/glossary.html#gloss_complex_functional_adaptation" target="_new">complex functional adaptations</a> composed of multiple subprocesses with specific chunks of <a class="thought" href="entries/brain_entry.html">brain</a> providing <a class="thought" href="entries/hardware_entry.html">hardware</a> support. Such <a class="thought" href="entries/complexity_entry.html">complexity</a> will not spontaneously materialize in <a class="thought" href="entries/source_code_entry.html">source code</a>, any more than complex dishes like pepperoni pizza will spontaneously begin growing on palm trees.</p>
<p><a class="thought" href="entries/reason_entry.html">Reason</a>ing by <a class="thought" href="entries/analogy_entry.html">analogy</a> with humans--"anthropomorphically"--is exactly the wrong way to think about Friendly <a class="thought" href="entries/ai_entry.html">AI</a>. Humans have a complex, intricate <a class="thought" href="entries/architecture_entry.html">architecture</a>. Some of it, from the perspective of a Friendly <a class="thought" href="entries/ai_entry.html">AI</a> <a class="thought" href="entries/program_entry.html">program</a>mer, is worth duplicating; some is decidedly <i>not</i> worth duplicating; some of it needs to be duplicated, but differently. Assuming that AIs automatically possess "negative" <a class="thought" href="entries/human_entry.html">human</a> functionality leads to expecting the wrong malfunctions; to focusing attention on the wrong problems. Assuming that AIs <i>automatically</i> possess beneficial <a class="thought" href="entries/human_entry.html">human</a> functionality means not taking the efforts required to deliberately duplicate that functionality.</p>
<p>Even worse is Hollywood's tendency to stereotype AIs as "<a class="thought" href="entries/machine_entry.html">machine</a>s," or for people to assume that an <a class="thought" href="entries/ai_entry.html">AI</a> would behave like, say, <a class="thought" href="entries/windows_entry.html">Windows</a> 98. A real <a class="thought" href="entries/ai_entry.html">AI</a> wouldn't be a <a class="thought" href="entries/computer_entry.html">computer</a> <a class="thought" href="entries/program_entry.html">program</a> any more than a <a class="thought" href="entries/human_entry.html">human</a> is an <a class="thought" href="entries/amoeba_entry.html">amoeba</a>; most of the <a class="thought" href="entries/complexity_entry.html">complexity</a> would be as far from the <a class="thought" href="entries/program_entry.html">program</a> level as a <a class="thought" href="entries/human_entry.html">human</a>'s <a class="thought" href="entries/complexity_entry.html">complexity</a> is distant from the cellular level. One of the most stereotypical characteristics of "<a class="thought" href="entries/machine_entry.html">machine</a>s," for example, is lack of self-awareness. A toaster does not know that it is a toaster; a toaster has no <a class="thought" href="entries/sense_entry.html">sense</a> of its own purpose, and will burn bread as readily as make toast. A true <a class="thought" href="entries/ai_entry.html">AI</a>, by contrast, could have complete <a class="thought" href="entries/access_entry.html">access</a> to its own <a class="thought" href="entries/source_code_entry.html">source code</a>--a level of self-awareness presently beyond <a class="thought" href="entries/human_entry.html">human</a> capability.</p>
<p>Certain other themes are also prevalent in the fictional and popular formulations of the question. In the traditional form, obedience is <i>imposed.</i> Some <a class="thought" href="entries/single_electron_transfer_entry.html">set</a> of goals, or orders, is held in place <i>against the resistance</i> of the <a class="thought" href="entries/ai_entry.html">AI</a>'s "innate <a class="thought" href="entries/nature_entry.html">nature</a>." This form is especially popular among fiction-writers, since the breakdown of the imposition provides ready-made fodder for the story. Again, this error derives from an attempt to relate to AIs in the same way as we would relate to humans. A <a class="thought" href="entries/human_entry.html">human</a> being comes with a prepackaged "innate <a class="thought" href="entries/nature_entry.html">nature</a>," and any relation to that <a class="thought" href="entries/human_entry.html">human</a> will be a relation to that innate <a class="thought" href="entries/nature_entry.html">nature</a>. You might say that the fictional formulations address the question of how humanity can deal with some particular <a class="thought" href="entries/nature_entry.html">nature</a>, while Friendly <a class="thought" href="entries/ai_entry.html">AI</a> asks how to <i>build</i> a <a class="thought" href="entries/nature_entry.html">nature</a>--one that we'll have no trouble relating to. The question is not one of <i>dominance</i> or even <i>coexistence</i> but rather <i>creation.</i> This is not a challenge that humans encounter when dealing with other humans.</p>
<p>Thus, a properly designed Friendly <a class="thought" href="entries/ai_entry.html">AI</a> does <i>not,</i> as in popular fiction, consist of endless safeguards and coercions stopping the <a class="thought" href="entries/ai_entry.html">AI</a> from doing this, or forcing the <a class="thought" href="entries/ai_entry.html">AI</a> to do that, or preventing the <a class="thought" href="entries/ai_entry.html">AI</a> from <a class="thought" href="entries/thinking_entry.html">thinking</a> certain <a class="thought" href="entries/thought_entry.html">thought</a>s, or protecting the goal <a class="thought" href="entries/system_entry.html">system</a> from modification. That would be pushing against a lack of resistance--like charging a locked door at full speed, only to find the door ajar. If the <a class="thought" href="entries/ai_entry.html">AI</a> ever stops <i>wanting</i> to be Friendly, you've already lost.</p>
<p>The idea that hostility does not <i>automatically</i> pop up (or at least, that hostility does not pop up in the way usually proposed) is <a class="thought" href="entries/basic_entry.html">basic</a> to the class of Friendship <a class="thought" href="entries/system_entry.html">system</a> described in "<a href="http://web.archive.org/web/20091027051655/http://singinst.org/CFAI/index.html" target="_new">Friendly </a><a class="thought" href="entries/ai_entry.html">AI</a>." You can find an in-depth defense of this conclusion in "<a href="http://web.archive.org/web/20091027051655/http://singinst.org/CFAI/anthro.html" target="_new">Beyond anthropomorphism</a>." Also, since we often hear questions of the form "But why wouldn't an <a class="thought" href="entries/ai_entry.html">AI</a>...?," you can find some fast answers to the top 13 questions of that form in the "<a href="http://web.archive.org/web/20091027051655/http://singinst.org/CFAI/info/indexfaq.html" target="_new">Frequently Asked Questions</a>."</p>
<p>Trying to impose Friendliness against resistance is charging an open door; it is assuming negative <a class="thought" href="entries/human_entry.html">human</a> functionality and guarding against the wrong malfunctions. A corresponding error exists for incorrectly assuming positive <a class="thought" href="entries/human_entry.html">human</a> functionality; the error of assuming that, in the absence of resistance, you can specify some arbitrary <a class="thought" href="entries/single_electron_transfer_entry.html">set</a> of goals and then walk away. For one thing, this is almost certainly the wrong attitude to take. One of the conclusions that can be drawn from Friendliness theory--a guiding <a class="thought" href="entries/heuristic_entry.html">heuristic</a>, perhaps, if not a first principle--is the idea that building a mind is not like building a tool. Tools can be used for whatever <a class="thought" href="entries/action_entry.html">action</a> the wielder likes; a mind can have a <a class="thought" href="entries/sense_entry.html">sense</a> of its own purpose, and can originate <a class="thought" href="entries/action_entry.html">action</a>s to achieve that purpose.</p><h1>Cognition About Goals</h1><p>In some ways, Friendly <a class="thought" href="entries/ai_entry.html">AI</a> is duplicating what humans would call "<a class="thought" href="entries/common_sense_entry.html">common sense</a>" in the domain of goals. Not common-<a class="thought" href="entries/sense_entry.html">sense</a> <a class="thought" href="entries/knowledge_entry.html">knowledge</a><i>;</i> common-<a class="thought" href="entries/sense_entry.html">sense</a> <i><a class="thought" href="entries/reason_entry.html">reason</a>ing.</i> Common-<a class="thought" href="entries/sense_entry.html">sense</a> <a class="thought" href="entries/reason_entry.html">reason</a>ing in factual domains is hardly something that can be taken for granted, but it is still a problem that will--of necessity--have already been solved by the <a class="thought" href="entries/time_entry.html">time</a> AIs can independently harm or benefit humanity. If humans say that the sky is blue, and the <a class="thought" href="entries/ai_entry.html">AI</a> (by browsing the Web, or by controlling a <a class="thought" href="entries/digital_entry.html">digital</a> camera) later finds out that the sky is only blue by day when not obscured by clouds, and is purple with white polka-dots at night, then the description of the color of the sky can be modified accordingly. In fact, it could be modified just by the humans realizing their mistake and providing the <a class="thought" href="entries/ai_entry.html">AI</a> with further <a class="thought" href="entries/information_entry.html">information</a> about the color of the sky.</p>
<p>Here, again, one distinguishes between tool-level AIs and true minds. A tool-level <a class="thought" href="entries/ai_entry.html">AI</a> simply has the naked fact, stored somewhere in <a class="thought" href="entries/memory_entry.html">memory</a>, that the sky is blue (1). The fact exists without any <a class="thought" href="entries/knowledge_entry.html">knowledge</a> as to its origin, or that the <a class="thought" href="entries/program_entry.html">program</a>mers put it there. To alter the concept, the <a class="thought" href="entries/program_entry.html">program</a>mers would reach in (perhaps while the <a class="thought" href="entries/ai_entry.html">AI</a> was shut off) and directly tweak the stored <a class="thought" href="entries/information_entry.html">information</a>. By contrast, a mind-level <a class="thought" href="entries/ai_entry.html">AI</a> would receive, as sensory <a class="thought" href="entries/information_entry.html">information</a>, the <a class="thought" href="entries/program_entry.html">program</a>mer typing in "the sky is blue." (Presumably the <a class="thought" href="entries/ai_entry.html">AI</a> already has real, grounded, useful <a class="thought" href="entries/knowledge_entry.html">knowledge</a> of what a "sky" is, and which color is "blue," or these are just meaningless words.) The sensed keystrokes "the sky is blue" are interpreted as being a meaningful statement by the <a class="thought" href="entries/program_entry.html">program</a>mer. The <a class="thought" href="entries/ai_entry.html">AI</a> estimates how likely the <a class="thought" href="entries/program_entry.html">program</a>mer is to know about the sky's color and assigns a certain probability to the hypothesis that the sky is blue, based on the sensory <a class="thought" href="entries/information_entry.html">information</a> that the <a class="thought" href="entries/program_entry.html">program</a>mer thinks the sky is blue (or at least, said the sky is blue). Later, this hypothesis can be confirmed and expanded by more direct tests.</p>
<p>If, the next day, the <a class="thought" href="entries/program_entry.html">program</a>mer says, "Wait a minute, the sky is purple at night," then the <a class="thought" href="entries/ai_entry.html">AI</a> will (presumably) change the hypothesis about the sky's color to reflect the new <a class="thought" href="entries/information_entry.html">information</a>. A nontrivial amount of common-<a class="thought" href="entries/sense_entry.html">sense</a> <a class="thought" href="entries/reason_entry.html">reason</a>ing is needed to make this change <i>for the right <a class="thought" href="entries/reason_entry.html">reason</a>s.</i> It requires that the <a class="thought" href="entries/ai_entry.html">AI</a> model <a class="thought" href="entries/program_entry.html">program</a>mers as knowing more today than they did yesterday, or that the <a class="thought" href="entries/ai_entry.html">AI</a> understand the idea of a <a class="thought" href="entries/program_entry.html">program</a>mer "spotting an error" and correcting it (the <a class="thought" href="entries/ai_entry.html">AI</a> modeling the <a class="thought" href="entries/program_entry.html">program</a>mer modeling the AI!). At a higher level, it implies a sophisticated understanding of causation and validity; a realization, by the <a class="thought" href="entries/ai_entry.html">AI</a>, that the only <a class="thought" href="entries/reason_entry.html">reason</a> it ever did believe the sky was blue was that the <a class="thought" href="entries/program_entry.html">program</a>mer said so, and that new <a class="thought" href="entries/information_entry.html">information</a> from the <a class="thought" href="entries/program_entry.html">program</a>mer should therefore override old.</p>
<p>These are some of the behaviors are analyzed at length in "<a href="http://web.archive.org/web/20091027051655/http://singinst.org/CFAI/index.html" target="_new">Friendly </a><a class="thought" href="entries/ai_entry.html">AI</a>." (Or, rather, the <a class="thought" href="entries/analog_entry.html">analog</a>ous behaviors are analyzed for goals.) The <a class="thought" href="entries/ai_entry.html">AI</a> has beliefs about the sky's color that are probabilistic rather than absolute, and can therefore conceive of the beliefs being "wrong," and can therefore expand and correct those beliefs. (Discussed in "<a href="http://web.archive.org/web/20091027051655/http://singinst.org/CFAI/info/glossary.html#gloss_external_reference_semantics" target="_new">External reference </a><a class="thought" href="entries/semantics_entry.html">semantics</a>.") The <a class="thought" href="entries/ai_entry.html">AI</a> understands that its beliefs about the sky are derived from <a class="thought" href="entries/human_entry.html">human</a>-affirmed <a class="thought" href="entries/information_entry.html">information</a>, and that these beliefs will likely be wrong if the humans have made a mistake, and will therefore pay attention to additional <a class="thought" href="entries/information_entry.html">information</a> or corrections provided by the humans. (Discussed in "<a href="http://web.archive.org/web/20091027051655/http://singinst.org/CFAI/info/glossary.html#gloss_causal_validity_semantics" target="_new">Causal validity </a><a class="thought" href="entries/semantics_entry.html">semantics</a>.")</p>
<p>The ability to learn and self-correct is one that can apply to goals as well... <i>as long as the </i><a class="thought" href="entries/ai_entry.html">AI</a><i> is created with that in mind.</i> The question is not whether the <a class="thought" href="entries/ai_entry.html">AI</a> has the cognitive <i>capability</i> to learn, but whether the <a class="thought" href="entries/ai_entry.html">AI</a> has the <i>desire</i> to learn. The ability to learn facts is not an <i>easy</i> problem for <a class="thought" href="entries/ai_entry.html">AI</a> <a class="thought" href="entries/research_entry.html">research</a>ers to solve, but it is a problem that must be solved <i>before</i> AIs have the capability to harm or benefit humanity. The ability to learn facts can carry over into the capability to learn goals--to be sensitive to the <a class="thought" href="entries/program_entry.html">program</a>mers intentions--only if the <a class="thought" href="entries/ai_entry.html">AI</a> <i>starts out</i> with the idea that goals are probabilistic and that their presence was <a class="thought" href="entries/human_entry.html">human</a>-caused.</p>
<p>The simplest case of a short-<a class="thought" href="entries/circuit_entry.html">circuit</a> would be an <a class="thought" href="entries/ai_entry.html">AI</a> that had an absolute, non-probabilistic supergoal for "painting cars green." Actually, this is, in itself, a mistake; for a true <a class="thought" href="entries/ai_entry.html">AI</a> that happens to work in a car factory, painting cars green should be a subgoal of producing cars, which is a subgoal of fulfilling people's desire for cars, which means fulfilling a volitional request, which is directly Friendly under the "volition-based Friendliness" formulation used in "<a href="http://web.archive.org/web/20091027051655/http://singinst.org/CFAI/index.html" target="_new">Friendly </a><a class="thought" href="entries/ai_entry.html">AI</a>."</p>
<p>We'll analyze the simpler case, though, in which "painting cars green" is a supergoal, and consider what happens when the factory decides that the <a class="thought" href="entries/ai_entry.html">AI</a> should paint the cars red instead of green (which you can take as metaphor for needing to tweak some aspect of volition-based Friendliness). If, for some <a class="thought" href="entries/reason_entry.html">reason</a>, this is a true (non-tool-level) <a class="thought" href="entries/ai_entry.html">AI</a>, and a <a href="http://web.archive.org/web/20091027051655/http://singinst.org/GISAI/paradigms/seedAI.html" target="_new">seed </a><a class="thought" href="entries/ai_entry.html">AI</a> capable of self-modification, then the <a class="thought" href="entries/ai_entry.html">AI</a> will--obviously--resist any attempt to change its supergoals. Why? Because, if the <a class="thought" href="entries/ai_entry.html">AI</a>'s supergoal should change, a consequence of that changed supergoal <a class="thought" href="entries/content_entry.html">content</a> would be that the <a class="thought" href="entries/ai_entry.html">AI</a> would take different <a class="thought" href="entries/action_entry.html">action</a>s; in this case, the <a class="thought" href="entries/ai_entry.html">AI</a> predicts that its <a class="thought" href="entries/future_entry.html">future</a> self would paint the cars red, instead of green. Since the <a class="thought" href="entries/ai_entry.html">AI</a>'s <i>current</i> goal is to paint cars green, changing the supergoal would thus be perceived as undesirable; would be predicted to lead to a lesser degree of supergoal fulfillment. This class of short-<a class="thought" href="entries/circuit_entry.html">circuit</a> failure is <i>not</i> inevitable; it requires (we currently think) a relatively small amount effort to design a <a href="http://web.archive.org/web/20091027051655/http://singinst.org/CFAI/design/friendly/index.html" target="_new">probabilistic goal </a><a class="thought" href="entries/architecture_entry.html">architecture</a>, or at least a small amount of effort compared to building a working goal <a class="thought" href="entries/architecture_entry.html">architecture</a> to begin with. The point is that the effort must be taken. The cognitive ability to conceive of a supergoal as being "corrected" is possible, perhaps relatively easy, but not <i>automatic.</i>
</p>
<p>The <a class="thought" href="entries/ai_entry.html">AI</a> doesn't need to start out with the specific idea that cars might need to be green--there is no need to <i>explicitly</i> anticipate everything in advance--but the <a class="thought" href="entries/ai_entry.html">AI</a> does need to start out with probabilistic supergoals. If the <a class="thought" href="entries/ai_entry.html">AI</a> has probabilistic supergoals, then this finite amount of <a class="thought" href="entries/complexity_entry.html">complexity</a> is sufficient to handle any color of the rainbow a car might need to be, no <a class="thought" href="entries/matter_entry.html">matter</a> how unexpected; it may even be sufficient to handle the transition to a real <a class="thought" href="entries/ai_entry.html">AI</a>, one that cares about people rather than cars, when the <a class="thought" href="entries/program_entry.html">program</a>mers finally wise up. If, however, the <a class="thought" href="entries/ai_entry.html">AI</a> conceives of its current supergoals as absolute, "correct by definition," such that nothing is processed as making a change desirable, then this not only prevents the <a class="thought" href="entries/switch_entry.html">switch</a> from green paint to red paint, or the <a class="thought" href="entries/switch_entry.html">switch</a> from car-painting to volitional Friendliness, it will also prevent the <a class="thought" href="entries/program_entry.html">program</a>mers from modifying the goal <a class="thought" href="entries/system_entry.html">system</a> to make supergoals probabilistic. The <a class="thought" href="entries/ai_entry.html">AI</a> will <i>try</i> to prevent the <a class="thought" href="entries/program_entry.html">program</a>mers from modifying it, anyway--an infantlike <a class="thought" href="entries/ai_entry.html">AI</a> is not likely to have much luck. Still, there's a stage of development beyond which an <a class="thought" href="entries/ai_entry.html">AI</a> needs certain architectural features. The <a class="thought" href="entries/ai_entry.html">AI</a> needs that <i><a class="thought" href="entries/basic_entry.html">basic</a></i> amount of <a class="thought" href="entries/complexity_entry.html">complexity</a> which is required to absorb <i>additional</i> <a class="thought" href="entries/complexity_entry.html">complexity</a>, and to see the acquisition of that <a class="thought" href="entries/complexity_entry.html">complexity</a> as desirable.</p>
<p>An adult <a class="thought" href="entries/human_entry.html">human</a> <a class="thought" href="entries/brain_entry.html">brain</a> contains a huge amount of <a class="thought" href="entries/data_entry.html">data</a>--a finite amount, but still an amount too large to be deliberately <a class="thought" href="entries/program_entry.html">program</a>med. However, all that <a class="thought" href="entries/data_entry.html">data</a> exists as a result of <a class="thought" href="entries/human_entry.html">human</a> <a class="thought" href="entries/learning_entry.html">learning</a>; the <i>means by which we learn</i> are much more compact than the learned <a class="thought" href="entries/data_entry.html">data</a>. And of course, we also <i>learn how to learn.</i> The upshot is that, even though the world is an enormously complex place, it may take only a finite amount of <a class="thought" href="entries/program_entry.html">program</a>mer effort to produce an <a class="thought" href="entries/ai_entry.html">AI</a> that can <i>grow into</i> understanding that world at least as well as a <a class="thought" href="entries/human_entry.html">human</a>. After all, it only took a finite amount of <a class="thought" href="entries/evolution_entry.html">evolution</a> to produce the 3 billion bases that comprise the 750-megabyte <a class="thought" href="entries/human_entry.html">human</a> <a class="thought" href="entries/genome_entry.html">genome</a>.</p>
<p>The <a class="thought" href="entries/architecture_entry.html">architecture</a>s described in "Friendly <a class="thought" href="entries/ai_entry.html">AI</a>" are a self-sustaining funnel through which certain kinds of <a class="thought" href="entries/complexity_entry.html">complexity</a> can be poured into an <a class="thought" href="entries/ai_entry.html">AI</a>, <i>such that the </i><a class="thought" href="entries/ai_entry.html">AI</a><i> perceives the pouring as desirable at any given point in <a class="thought" href="entries/time_entry.html">time</a>.</i> There's more to it than probabilistic supergoals--that was just one example of a kind of structural <a class="thought" href="entries/complexity_entry.html">complexity</a> that humans take for granted--but the list is, nonetheless, finite. It only takes a finite amount of understanding to see the need for any additional understanding that becomes necessary.</p><h1>Today, Not tomorrow</h1><p>As best as we can currently figure, the amount of effort needed to create a Friendly <a class="thought" href="entries/ai_entry.html">AI</a> is small relative to the effort needed to create <a class="thought" href="entries/ai_entry.html">AI</a> in the first place. But it's a <i>very important</i> effort. It's a critical link for the entire <a class="thought" href="entries/human_entry.html">human</a> <a class="thought" href="entries/species_entry.html">species</a>.</p>
<p>It's <i>not</i> too early to start <a class="thought" href="entries/thinking_entry.html">thinking</a> about it, no <a class="thought" href="entries/matter_entry.html">matter</a> how primitive current AIs are. To predict that <a class="thought" href="entries/ai_entry.html">AI</a> will arrive in thirty years is conservative for <a class="thought" href="entries/futurist_entry.html">futurist</a>s; to predict that Friendly <a class="thought" href="entries/ai_entry.html">AI</a> will be <i>required</i> in five years is conservative for a Friendliness <a class="thought" href="entries/research_entry.html">research</a>er. To predict that the first generally intelligent AIs will be comically stupid is conservative for an <a class="thought" href="entries/ai_entry.html">AI</a> <a class="thought" href="entries/research_entry.html">research</a>er; to predict that the first generally intelligent AIs may have the <a class="thought" href="entries/intelligence_entry.html">intelligence</a> to benefit or harm humans is conservative for a Friendliness <a class="thought" href="entries/research_entry.html">research</a>er. Also, some architectural features may need to be adopted early on, to prevent an unworkable <a class="thought" href="entries/architecture_entry.html">architecture</a> from being entrenched in an infant <a class="thought" href="entries/ai_entry.html">AI</a> that later begins moving toward general <a class="thought" href="entries/intelligence_entry.html">intelligence</a>. The <a class="thought" href="entries/analogy_entry.html">analogy</a> would be to a <a class="thought" href="entries/y2k_entry.html">Y2K</a> <a class="thought" href="entries/bug_entry.html">bug</a>--representing four-digit years is trivial if you think of it in advance, but very costly if you think of it afterwards.</p>
<p>Combining these two considerations may even bring Friendly <a class="thought" href="entries/ai_entry.html">AI</a> within reach of "things to actually worry about today." It is beyond doubt that no current <a class="thought" href="entries/ai_entry.html">AI</a> project has achieved real AI; all current AIs are tools, and do not make independent decisions that could harm or benefit humans. Similarly, the current scientific consensus seems to be that no present-day project has the potential to eventually grow into a true <a class="thought" href="entries/ai_entry.html">AI</a>. Some of the <a class="thought" href="entries/research_entry.html">research</a>ers working on those projects, though, say otherwise--<i>and it is "conservative" for a Friendliness <a class="thought" href="entries/research_entry.html">research</a>er to believe them,</i> even if his personal theory of <a class="thought" href="entries/ai_entry.html">AI</a> says that these projects probably won't succeed.</p>
<p>Of course, an utterly bankrupt project is likely to be too simple to implement even the most <a class="thought" href="entries/basic_entry.html">basic</a> features of Friendliness, and such projects are beyond the responsibility of even a "conservative" Friendliness <a class="thought" href="entries/research_entry.html">research</a>er to worry about, no <a class="thought" href="entries/matter_entry.html">matter</a> what pronouncements are made about them. But why not say that--for example--if a project has a sufficiently general <a class="thought" href="entries/architecture_entry.html">architecture</a> to <i>represent</i> probabilistic supergoals, then that <a class="thought" href="entries/architecture_entry.html">architecture</a> probably <i>should</i> use probabilistic supergoals? It's not much additional effort, compared to implementing a goal <a class="thought" href="entries/system_entry.html">system</a> in the first place. Of course, SIAI knows of only one current project advanced enough to even <i>begin</i> implementing the first baby steps toward Friendliness--but where there is one today, there may be a dozen tomorrow. The <a class="thought" href="entries/singularity_entry.html">Singularity</a> Institute's belief that true <a class="thought" href="entries/ai_entry.html">AI</a> can be created in ten years is confessedly unconservative, but not our belief that Friendly <a class="thought" href="entries/ai_entry.html">AI</a> should be done "today, not tomorrow."</p>
<p>Friendly <a class="thought" href="entries/ai_entry.html">AI</a> is also important insofar as present-day society has begun debating the peril and promise of advanced <a class="thought" href="entries/technology_entry.html">technology</a>. The field is not advanced enough to pronounce with certainty that Friendly <a class="thought" href="entries/ai_entry.html">AI</a> can be created; nonetheless, we can say that, at the moment, it looks possible, and that certain commonly advanced objections are either completely unrealistic or extremely improbable. Thus, a very strong case can be made that--out of all the advanced technologies being debated--Friendly <a class="thought" href="entries/ai_entry.html">AI</a> is the best <a class="thought" href="entries/technology_entry.html">technology</a> to develop <i>first.</i> <a class="thought" href="entries/ai_entry.html">Artificial Intelligence</a> is the only one of our <a class="thought" href="entries/invention_entry.html">invention</a>s that can, in theory, be given a <a class="thought" href="entries/conscience_entry.html">conscience</a>. Success in developing Friendly <a class="thought" href="entries/ai_entry.html">AI</a> is more likely to help humanity safely develop <a class="thought" href="entries/nanotechnology_entry.html">nanotechnology</a> than the other way around. Similarly, <a href="http://web.archive.org/web/20091027051655/http://singinst.org/CFAI/policy.html#comparative" target="_new">comparative analysis</a> of <a href="http://web.archive.org/web/20091027051655/http://singinst.org/CFAI/index.html" target="_new">Friendly </a><a class="thought" href="entries/ai_entry.html">AI</a> <a href="http://web.archive.org/web/20091027051655/http://singinst.org/CFAI/policy.html#comparative_fai" target="_new"> relative to computing power</a> suggests that the difficulty of creating <a class="thought" href="entries/ai_entry.html">AI</a> <i>decreases</i> with increasing computing power, while the difficulty of Friendly <a class="thought" href="entries/ai_entry.html">AI</a> does not decrease; thus, it is unwise to hold off too long on creating Friendly <a class="thought" href="entries/ai_entry.html">AI</a>. In this way, the theoretical background provided by present-day <a class="thought" href="entries/knowledge_entry.html">knowledge</a> of Friendly <a class="thought" href="entries/ai_entry.html">AI</a> can be relevant to present-day decisions.</p>
<p>For more <a class="thought" href="entries/information_entry.html">information</a>, see the <a href="http://web.archive.org/web/20091027051655/http://singinst.org/CFAI/index.html" target="_new">book-length treatment in "Friendly</a> <a class="thought" href="entries/ai_entry.html">AI</a>," available on our website.</p>
<a name="r1"></a>
<p class="Reference">1. See "<a href="http://web.archive.org/web/20091027051655/http://singinst.org/GISAI/paradigms/thinking.html" target="_new">1.2:</a> <a class="thought" href="entries/thinking_entry.html">Thinking</a> <a href="http://web.archive.org/web/20091027051655/http://singinst.org/GISAI/paradigms/thinking.html" target="_new">About</a> <a class="thought" href="entries/ai_entry.html">AI</a>" for an explanation of what's needed before the <a class="thought" href="entries/ai_entry.html">AI</a> can actually be said to know, in any meaningful <a class="thought" href="entries/sense_entry.html">sense</a>, that the sky is blue, rather than just having a semantic net containing a <a class="thought" href="entries/bit_entry.html">bit</a> of predicate <a class="thought" href="entries/calculus_entry.html">calculus</a>. For example, an <a class="thought" href="entries/ai_entry.html">AI</a> that had a remote-controlled <a class="thought" href="entries/digital_entry.html">digital</a> camera and which understood the concept of ambient lighting, reflectance spectrums, and color constancy, and which used <a class="thought" href="entries/information_entry.html">information</a> about the sky's color to decide whether the object seen today is the same object that was seen at night, would meaningfully understand that the sky was blue. Likewise an <a class="thought" href="entries/ai_entry.html">AI</a> that had a sensory modality similar to the <a class="thought" href="entries/human_entry.html">human</a> visual cortex--including derived properties such as hue, saturation, and texture rather than just RGB pixels--might understand something about the pure, intense blue of a cloudless sky. Having a couple of <a href="http://web.archive.org/web/20091027051655/http://singinst.org/GISAI/meta/glossary.html#gloss_lisp_tokens" target="_new">suggestively named</a> <a class="thought" href="entries/lisp_entry.html">LISP</a> <a href="http://web.archive.org/web/20091027051655/http://singinst.org/GISAI/meta/glossary.html#gloss_lisp_tokens" target="_new">tokens</a> or <a class="thought" href="entries/java_entry.html">Java</a> objects asserting is-color(sky, blue) actually contains only the <a class="thought" href="entries/information_entry.html">information</a> G0023(G0024, G0025).</p>
</td><td>&#160;</td><td valign="top"><a href="#discussion">Join the discussion about this article on Mind&#183;X!</a><p></p></td><td> &#160; </td>
</tr>
<tr><td colspan="6"><img alt="" border="0" height="35" src="https://web.archive.org/web/20091027051655im_/http://www.kurzweilai.net/blank.gif" width="35"></td></tr>
<tr>
<td>&#160;</td>
<td colspan="4">
<a name="discussion"></a><p><span class="mindxheader">&#160;&#160;&#160;[<a href="https://web.archive.org/web/20091027051655/http://www.kurzweilai.net/mindx/frame.html?main=post.php?reply%3D1556" target="_top">Post New Comment</a>]<br>&#160;&#160;&#160;</span>Mind&#183;X Discussion About This Article:</p><a name="id1557"></a>
<table border="0" cellpadding="5" cellspacing="0" width="100%">
<tr>
<td><img height="1" src="https://web.archive.org/web/20091027051655im_/http://www.kurzweilai.net/images/blank.gif" width="0"></td>
<td colspan="4"><img height="1" src="https://web.archive.org/web/20091027051655im_/http://www.kurzweilai.net/images/blank.gif" width="679"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20091027051655im_/http://www.kurzweilai.net/mindx/images/round-left.gif"></td>
<td bgcolor="#CCCCCC"><p>Yudkowsky on What is Friendly AI?<br><span class="mindxheader"><i>posted on 07/28/2001 5:28 AM by jjaeger@mecfilms.com</i></span></p></td>
<td align="right" bgcolor="#CCCCCC" valign="top"><span class="mindxheader">[<a href="#discussion">Top</a>]<br>[<a href="https://web.archive.org/web/20091027051655/http://www.kurzweilai.net/mindx/frame.html?main=show_thread.php?rootID%3D1556%23id1557" target="_top">Mind&#183;X</a>]<br>[<a href="https://web.archive.org/web/20091027051655/http://www.kurzweilai.net/mindx/frame.html?main=post.php?reply%3D1557" target="_top">Reply to this post</a>]</span></td>
<td align="right" bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20091027051655im_/http://www.kurzweilai.net/mindx/images/round-right.gif"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#DDDDDD" colspan="4"><p>I completely agree with you Eliezer about Hollywood's depiction of AI: they have it all wrong.  I am just finishing up a book called ALIEN INTELLIGENCE by James Martin and he goes into some real depth on this subject as well. 
<br>
<br>
As I said earlier, I first read Eliezer's writings about five years ago (when he was in his early 20's even). . . and IMO, this guy is one of the most brilliant thinkers on AI and the Singularity I know of.
<br>
<br>
James Jaeger
<br>
<br>
P.S. Eliezer, if you ever write a sci-fi screenplay, I hope you will consider contacting me first.  http://www.mecfilms.com/jrjbio.htm
<br>
</p></td>
</tr>
<tr>
<td><img height="1" src="https://web.archive.org/web/20091027051655im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20091027051655im_/http://www.kurzweilai.net/mindx/images/round-bottom-left.gif"></td>
<td bgcolor="#DDDDDD" colspan="2"><img height="1" src="https://web.archive.org/web/20091027051655im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td align="right" bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20091027051655im_/http://www.kurzweilai.net/mindx/images/round-bottom-right.gif"></td>
</tr>
</table>
<a name="id1673"></a>
<table border="0" cellpadding="5" cellspacing="0" width="100%">
<tr>
<td><img height="1" src="https://web.archive.org/web/20091027051655im_/http://www.kurzweilai.net/images/blank.gif" width="20"></td>
<td colspan="4"><img height="1" src="https://web.archive.org/web/20091027051655im_/http://www.kurzweilai.net/images/blank.gif" width="659"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20091027051655im_/http://www.kurzweilai.net/mindx/images/round-left.gif"></td>
<td bgcolor="#CCCCCC"><p>Re: Yudkowsky on What is Friendly AI?<br><span class="mindxheader"><i>posted on 08/01/2001 5:48 PM by Joe@smith.com</i></span></p></td>
<td align="right" bgcolor="#CCCCCC" valign="top"><span class="mindxheader">[<a href="#discussion">Top</a>]<br>[<a href="https://web.archive.org/web/20091027051655/http://www.kurzweilai.net/mindx/frame.html?main=show_thread.php?rootID%3D1556%23id1673" target="_top">Mind&#183;X</a>]<br>[<a href="https://web.archive.org/web/20091027051655/http://www.kurzweilai.net/mindx/frame.html?main=post.php?reply%3D1673" target="_top">Reply to this post</a>]</span></td>
<td align="right" bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20091027051655im_/http://www.kurzweilai.net/mindx/images/round-right.gif"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#DDDDDD" colspan="4"><p>For the record, not that it's important, I believe Mr. Yudkowsky is 21 now, making him 16 five years ago.</p></td>
</tr>
<tr>
<td><img height="1" src="https://web.archive.org/web/20091027051655im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20091027051655im_/http://www.kurzweilai.net/mindx/images/round-bottom-left.gif"></td>
<td bgcolor="#DDDDDD" colspan="2"><img height="1" src="https://web.archive.org/web/20091027051655im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td align="right" bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20091027051655im_/http://www.kurzweilai.net/mindx/images/round-bottom-right.gif"></td>
</tr>
</table>
<a name="id6382"></a>
<table border="0" cellpadding="5" cellspacing="0" width="100%">
<tr>
<td><img height="1" src="https://web.archive.org/web/20091027051655im_/http://www.kurzweilai.net/images/blank.gif" width="40"></td>
<td colspan="4"><img height="1" src="https://web.archive.org/web/20091027051655im_/http://www.kurzweilai.net/images/blank.gif" width="639"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20091027051655im_/http://www.kurzweilai.net/mindx/images/round-left.gif"></td>
<td bgcolor="#CCCCCC"><p>Re: Yudkowsky on What is Friendly AI?<br><span class="mindxheader"><i>posted on 04/25/2002 1:16 AM by Citizen Blue</i></span></p></td>
<td align="right" bgcolor="#CCCCCC" valign="top"><span class="mindxheader">[<a href="#discussion">Top</a>]<br>[<a href="https://web.archive.org/web/20091027051655/http://www.kurzweilai.net/mindx/frame.html?main=show_thread.php?rootID%3D1556%23id6382" target="_top">Mind&#183;X</a>]<br>[<a href="https://web.archive.org/web/20091027051655/http://www.kurzweilai.net/mindx/frame.html?main=post.php?reply%3D6382" target="_top">Reply to this post</a>]</span></td>
<td align="right" bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20091027051655im_/http://www.kurzweilai.net/mindx/images/round-right.gif"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#DDDDDD" colspan="4"><p>I think part of friendly AI is not having to go through the same evolutionary period that sentient creatures have done and by different constructs no primal mind was ever needed.  </p></td>
</tr>
<tr>
<td><img height="1" src="https://web.archive.org/web/20091027051655im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20091027051655im_/http://www.kurzweilai.net/mindx/images/round-bottom-left.gif"></td>
<td bgcolor="#DDDDDD" colspan="2"><img height="1" src="https://web.archive.org/web/20091027051655im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td align="right" bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20091027051655im_/http://www.kurzweilai.net/mindx/images/round-bottom-right.gif"></td>
</tr>
</table>
<a name="id6894"></a>
<table border="0" cellpadding="5" cellspacing="0" width="100%">
<tr>
<td><img height="1" src="https://web.archive.org/web/20091027051655im_/http://www.kurzweilai.net/images/blank.gif" width="60"></td>
<td colspan="4"><img height="1" src="https://web.archive.org/web/20091027051655im_/http://www.kurzweilai.net/images/blank.gif" width="619"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20091027051655im_/http://www.kurzweilai.net/mindx/images/round-left.gif"></td>
<td bgcolor="#CCCCCC"><p>Re: Yudkowsky on What is Friendly AI?<br><span class="mindxheader"><i>posted on 05/17/2002 1:55 PM by altima@yifan.net</i></span></p></td>
<td align="right" bgcolor="#CCCCCC" valign="top"><span class="mindxheader">[<a href="#discussion">Top</a>]<br>[<a href="https://web.archive.org/web/20091027051655/http://www.kurzweilai.net/mindx/frame.html?main=show_thread.php?rootID%3D1556%23id6894" target="_top">Mind&#183;X</a>]<br>[<a href="https://web.archive.org/web/20091027051655/http://www.kurzweilai.net/mindx/frame.html?main=post.php?reply%3D6894" target="_top">Reply to this post</a>]</span></td>
<td align="right" bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20091027051655im_/http://www.kurzweilai.net/mindx/images/round-right.gif"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#DDDDDD" colspan="4"><p>Yup.</p></td>
</tr>
<tr>
<td><img height="1" src="https://web.archive.org/web/20091027051655im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20091027051655im_/http://www.kurzweilai.net/mindx/images/round-bottom-left.gif"></td>
<td bgcolor="#DDDDDD" colspan="2"><img height="1" src="https://web.archive.org/web/20091027051655im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td align="right" bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20091027051655im_/http://www.kurzweilai.net/mindx/images/round-bottom-right.gif"></td>
</tr>
</table>
<a name="id13376"></a>
<table border="0" cellpadding="5" cellspacing="0" width="100%">
<tr>
<td><img height="1" src="https://web.archive.org/web/20091027051655im_/http://www.kurzweilai.net/images/blank.gif" width="80"></td>
<td colspan="4"><img height="1" src="https://web.archive.org/web/20091027051655im_/http://www.kurzweilai.net/images/blank.gif" width="599"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20091027051655im_/http://www.kurzweilai.net/mindx/images/round-left.gif"></td>
<td bgcolor="#CCCCCC"><p>Re: Yudkowsky on What is Friendly AI?<br><span class="mindxheader"><i>posted on 01/11/2003 1:30 PM by <a href="http://web.archive.org/web/20091027051655/mailto:trait70426@aol.com">harold macdonald</a></i></span></p></td>
<td align="right" bgcolor="#CCCCCC" valign="top"><span class="mindxheader">[<a href="#discussion">Top</a>]<br>[<a href="https://web.archive.org/web/20091027051655/http://www.kurzweilai.net/mindx/frame.html?main=show_thread.php?rootID%3D1556%23id13376" target="_top">Mind&#183;X</a>]<br>[<a href="https://web.archive.org/web/20091027051655/http://www.kurzweilai.net/mindx/frame.html?main=post.php?reply%3D13376" target="_top">Reply to this post</a>]</span></td>
<td align="right" bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20091027051655im_/http://www.kurzweilai.net/mindx/images/round-right.gif"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#DDDDDD" colspan="4"><p>Kurzweil tries to paint everything rosy as hell. I think he is trying to attract funding.  Vinge, I think, is a little bit more realistic.  A good analogy is the relationship between animals and humans.  I love my dog, however Japanese scientists have attached the reanimated heads of mice onto the thighs of a living mouse.  I bet Yudkowski would not want to end up in one of the Machine early prototype surgical labs!!  
<br>
</p></td>
</tr>
<tr>
<td><img height="1" src="https://web.archive.org/web/20091027051655im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20091027051655im_/http://www.kurzweilai.net/mindx/images/round-bottom-left.gif"></td>
<td bgcolor="#DDDDDD" colspan="2"><img height="1" src="https://web.archive.org/web/20091027051655im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td align="right" bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20091027051655im_/http://www.kurzweilai.net/mindx/images/round-bottom-right.gif"></td>
</tr>
</table>
<a name="id13378"></a>
<table border="0" cellpadding="5" cellspacing="0" width="100%">
<tr>
<td><img height="1" src="https://web.archive.org/web/20091027051655im_/http://www.kurzweilai.net/images/blank.gif" width="100"></td>
<td colspan="4"><img height="1" src="https://web.archive.org/web/20091027051655im_/http://www.kurzweilai.net/images/blank.gif" width="579"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20091027051655im_/http://www.kurzweilai.net/mindx/images/round-left.gif"></td>
<td bgcolor="#CCCCCC"><p>Re: Yudkowsky on What is Friendly AI?<br><span class="mindxheader"><i>posted on 01/11/2003 1:52 PM by <a href="http://web.archive.org/web/20091027051655/mailto:trait70426@aol.com">harold macdonald</a></i></span></p></td>
<td align="right" bgcolor="#CCCCCC" valign="top"><span class="mindxheader">[<a href="#discussion">Top</a>]<br>[<a href="https://web.archive.org/web/20091027051655/http://www.kurzweilai.net/mindx/frame.html?main=show_thread.php?rootID%3D1556%23id13378" target="_top">Mind&#183;X</a>]<br>[<a href="https://web.archive.org/web/20091027051655/http://www.kurzweilai.net/mindx/frame.html?main=post.php?reply%3D13378" target="_top">Reply to this post</a>]</span></td>
<td align="right" bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20091027051655im_/http://www.kurzweilai.net/mindx/images/round-right.gif"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#DDDDDD" colspan="4"><p>I have read the entire Kurzweil site.  It is a beautiful educational resource. A regular guy like me got a really good education in emergent tech from the MIT faculty and their associated advisors from around the world.  The thing which participants really need to know is that involuntary human brain scanning has been going on for a long time.  Please witness:
<br>
<br>
www.mindcontrolforums.com
<br>
<br>
[note the plural]
<br>
<br>
<br>
and Citizens Against Human Rights Abuse, otherwise known as C.A.H.R.A.  who are at:
<br>
<br>
http://www.dcn.davis.ca.us/~welsh/
<br>
<br>
<br>
can you dig it?
<br>
hm 
<br>
<br>
post script
<br>
i challenge the site monitors not to remove this.  you yourselves could wind up in "machine" labs, and should not make light of it.</p></td>
</tr>
<tr>
<td><img height="1" src="https://web.archive.org/web/20091027051655im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20091027051655im_/http://www.kurzweilai.net/mindx/images/round-bottom-left.gif"></td>
<td bgcolor="#DDDDDD" colspan="2"><img height="1" src="https://web.archive.org/web/20091027051655im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td align="right" bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20091027051655im_/http://www.kurzweilai.net/mindx/images/round-bottom-right.gif"></td>
</tr>
</table>
<a name="id13393"></a>
<table border="0" cellpadding="5" cellspacing="0" width="100%">
<tr>
<td><img height="1" src="https://web.archive.org/web/20091027051655im_/http://www.kurzweilai.net/images/blank.gif" width="120"></td>
<td colspan="4"><img height="1" src="https://web.archive.org/web/20091027051655im_/http://www.kurzweilai.net/images/blank.gif" width="559"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20091027051655im_/http://www.kurzweilai.net/mindx/images/round-left.gif"></td>
<td bgcolor="#CCCCCC"><p>Re: Yudkowsky on What is Friendly AI?<br><span class="mindxheader"><i>posted on 01/12/2003 1:01 AM by ELDRAS</i></span></p></td>
<td align="right" bgcolor="#CCCCCC" valign="top"><span class="mindxheader">[<a href="#discussion">Top</a>]<br>[<a href="https://web.archive.org/web/20091027051655/http://www.kurzweilai.net/mindx/frame.html?main=show_thread.php?rootID%3D1556%23id13393" target="_top">Mind&#183;X</a>]<br>[<a href="https://web.archive.org/web/20091027051655/http://www.kurzweilai.net/mindx/frame.html?main=post.php?reply%3D13393" target="_top">Reply to this post</a>]</span></td>
<td align="right" bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20091027051655im_/http://www.kurzweilai.net/mindx/images/round-right.gif"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#DDDDDD" colspan="4"><p>Harold,
<br>
<br>
you're right of course.
<br>
<br>
I'm a member of the scanning unit bt the way, and have your brain scan records in front of me.
<br>
<br>
This looks liike it was taken on Tuesday last.
<br>
<br>
you were thinking about the girl who serves coffee in that cafe you go to.
<br>
<br>
<br>
Don't bother, i have her scans here. she's  LESBIAN INTO BEATINGS.
<br>
there's a better match for you in Chigago. it IS in the zoo, and a bit hairy, but your  brain scans match exactly.
<br>
<br>
the is no charge for this service. 
<br>
<br>
just don't tell THEM.
<br>
<br>
ELDRAS
<br>
<br>
http://www.geocities.com/john_f_ellis/bess.htm</p></td>
</tr>
<tr>
<td><img height="1" src="https://web.archive.org/web/20091027051655im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20091027051655im_/http://www.kurzweilai.net/mindx/images/round-bottom-left.gif"></td>
<td bgcolor="#DDDDDD" colspan="2"><img height="1" src="https://web.archive.org/web/20091027051655im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td align="right" bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20091027051655im_/http://www.kurzweilai.net/mindx/images/round-bottom-right.gif"></td>
</tr>
</table>
<a name="id13407"></a>
<table border="0" cellpadding="5" cellspacing="0" width="100%">
<tr>
<td><img height="1" src="https://web.archive.org/web/20091027051655im_/http://www.kurzweilai.net/images/blank.gif" width="140"></td>
<td colspan="4"><img height="1" src="https://web.archive.org/web/20091027051655im_/http://www.kurzweilai.net/images/blank.gif" width="539"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20091027051655im_/http://www.kurzweilai.net/mindx/images/round-left.gif"></td>
<td bgcolor="#CCCCCC"><p>Re: Yudkowsky on What is Friendly AI?<br><span class="mindxheader"><i>posted on 01/12/2003 1:56 PM by <a href="http://web.archive.org/web/20091027051655/mailto:trait70426@aol.com">harold macdonald</a></i></span></p></td>
<td align="right" bgcolor="#CCCCCC" valign="top"><span class="mindxheader">[<a href="#discussion">Top</a>]<br>[<a href="https://web.archive.org/web/20091027051655/http://www.kurzweilai.net/mindx/frame.html?main=show_thread.php?rootID%3D1556%23id13407" target="_top">Mind&#183;X</a>]<br>[<a href="https://web.archive.org/web/20091027051655/http://www.kurzweilai.net/mindx/frame.html?main=post.php?reply%3D13407" target="_top">Reply to this post</a>]</span></td>
<td align="right" bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20091027051655im_/http://www.kurzweilai.net/mindx/images/round-right.gif"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#DDDDDD" colspan="4"><p>Well lemme see now.  If scientific community has telepathically penetrated my cognitive sphere, what privacy do I have?  Why should I bother to capitalize I?  I no longer meaningfully exist.  i know it sounds laughable, but they have done it to me.  And, kind sir, they can do it to You.  End of self.  yourself.</p></td>
</tr>
<tr>
<td><img height="1" src="https://web.archive.org/web/20091027051655im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20091027051655im_/http://www.kurzweilai.net/mindx/images/round-bottom-left.gif"></td>
<td bgcolor="#DDDDDD" colspan="2"><img height="1" src="https://web.archive.org/web/20091027051655im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td align="right" bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20091027051655im_/http://www.kurzweilai.net/mindx/images/round-bottom-right.gif"></td>
</tr>
</table>
<a name="id13470"></a>
<table border="0" cellpadding="5" cellspacing="0" width="100%">
<tr>
<td><img height="1" src="https://web.archive.org/web/20091027051655im_/http://www.kurzweilai.net/images/blank.gif" width="160"></td>
<td colspan="4"><img height="1" src="https://web.archive.org/web/20091027051655im_/http://www.kurzweilai.net/images/blank.gif" width="519"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20091027051655im_/http://www.kurzweilai.net/mindx/images/round-left.gif"></td>
<td bgcolor="#CCCCCC"><p>Re: Yudkowsky on What is Friendly AI?<br><span class="mindxheader"><i>posted on 01/15/2003 12:25 AM by ELDRAS</i></span></p></td>
<td align="right" bgcolor="#CCCCCC" valign="top"><span class="mindxheader">[<a href="#discussion">Top</a>]<br>[<a href="https://web.archive.org/web/20091027051655/http://www.kurzweilai.net/mindx/frame.html?main=show_thread.php?rootID%3D1556%23id13470" target="_top">Mind&#183;X</a>]<br>[<a href="https://web.archive.org/web/20091027051655/http://www.kurzweilai.net/mindx/frame.html?main=post.php?reply%3D13470" target="_top">Reply to this post</a>]</span></td>
<td align="right" bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20091027051655im_/http://www.kurzweilai.net/mindx/images/round-right.gif"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#DDDDDD" colspan="4"><p>Hi
<br>
<br>
Well we want you to join us spying on others.
<br>
<br>
Yudkowsky is a bit highly charged, at least the last email exchange i had with him was.
<br>
<br>
Ben (Goertzel) knows him better than me.
<br>
<br>
Hey do you know:
<br>
<br>
www.extropy.org ?
<br>
<br>
ELDRAS</p></td>
</tr>
<tr>
<td><img height="1" src="https://web.archive.org/web/20091027051655im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20091027051655im_/http://www.kurzweilai.net/mindx/images/round-bottom-left.gif"></td>
<td bgcolor="#DDDDDD" colspan="2"><img height="1" src="https://web.archive.org/web/20091027051655im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td align="right" bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20091027051655im_/http://www.kurzweilai.net/mindx/images/round-bottom-right.gif"></td>
</tr>
</table>
<a name="id24222"></a>
<table border="0" cellpadding="5" cellspacing="0" width="100%">
<tr>
<td><img height="1" src="https://web.archive.org/web/20091027051655im_/http://www.kurzweilai.net/images/blank.gif" width="0"></td>
<td colspan="4"><img height="1" src="https://web.archive.org/web/20091027051655im_/http://www.kurzweilai.net/images/blank.gif" width="679"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20091027051655im_/http://www.kurzweilai.net/mindx/images/round-left.gif"></td>
<td bgcolor="#CCCCCC"><p>"Monsters of the Id" ( AI mustn't have bio-substrate)<br><span class="mindxheader"><i>posted on 03/07/2004 6:01 AM by <a href="https://web.archive.org/web/20091027051655/http://www.kurzweilai.net/mindx/profile.php?id=1115">Redakteur</a></i></span></p></td>
<td align="right" bgcolor="#CCCCCC" valign="top"><span class="mindxheader">[<a href="#discussion">Top</a>]<br>[<a href="https://web.archive.org/web/20091027051655/http://www.kurzweilai.net/mindx/frame.html?main=show_thread.php?rootID%3D1556%23id24222" target="_top">Mind&#183;X</a>]<br>[<a href="https://web.archive.org/web/20091027051655/http://www.kurzweilai.net/mindx/frame.html?main=post.php?reply%3D24222" target="_top">Reply to this post</a>]</span></td>
<td align="right" bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20091027051655im_/http://www.kurzweilai.net/mindx/images/round-right.gif"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#DDDDDD" colspan="4"><p>Although in this article it is no where explicitly stated, when reassuring us that AIs would possess such qualities as benevolence or belligerence only if humans go out of their way to implement them, it seems to me that Yudkowsky is tacitly assuming that the AIs in question will 1) consist solely of non-biological hardware (i.e., they will contain no biological components, e.g. a chunk of brain tissue), 2) possess only software coded "from scratch" (i.e., feature no self-evolving / self-evolving algorithms and no architecture derived from brain scans). It almost goes without saying that the AIs in question could also not be artificially-enhanced human intelligences.
<br>
Otherwise, AIs could by all means still harbor "monsters of the Id" of which their programmers/builders have no inkling.</p></td>
</tr>
<tr>
<td><img height="1" src="https://web.archive.org/web/20091027051655im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20091027051655im_/http://www.kurzweilai.net/mindx/images/round-bottom-left.gif"></td>
<td bgcolor="#DDDDDD" colspan="2"><img height="1" src="https://web.archive.org/web/20091027051655im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td align="right" bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20091027051655im_/http://www.kurzweilai.net/mindx/images/round-bottom-right.gif"></td>
</tr>
</table>
<a name="id42446"></a>
<table border="0" cellpadding="5" cellspacing="0" width="100%">
<tr>
<td><img height="1" src="https://web.archive.org/web/20091027051655im_/http://www.kurzweilai.net/images/blank.gif" width="20"></td>
<td colspan="4"><img height="1" src="https://web.archive.org/web/20091027051655im_/http://www.kurzweilai.net/images/blank.gif" width="659"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20091027051655im_/http://www.kurzweilai.net/mindx/images/round-left.gif"></td>
<td bgcolor="#CCCCCC"><p>Re: "Monsters of the Id" ( AI mustn't have bio-substrate)<br><span class="mindxheader"><i>posted on 07/06/2005 8:19 PM by <a href="https://web.archive.org/web/20091027051655/http://www.kurzweilai.net/mindx/profile.php?id=471">eldras</a></i></span></p></td>
<td align="right" bgcolor="#CCCCCC" valign="top"><span class="mindxheader">[<a href="#discussion">Top</a>]<br>[<a href="https://web.archive.org/web/20091027051655/http://www.kurzweilai.net/mindx/frame.html?main=show_thread.php?rootID%3D1556%23id42446" target="_top">Mind&#183;X</a>]<br>[<a href="https://web.archive.org/web/20091027051655/http://www.kurzweilai.net/mindx/frame.html?main=post.php?reply%3D42446" target="_top">Reply to this post</a>]</span></td>
<td align="right" bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20091027051655im_/http://www.kurzweilai.net/mindx/images/round-right.gif"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#DDDDDD" colspan="4"><p>I've got to re read Elizer's stuff.
<br>
<br>
If there's any chance of building goals into emergent A.i. I need to know that.
<br>
<br>
I've really thought about it and cant see it's possible.
<br>
<br>
The Hawking Protocol
<br>
<br>
of A.I. safety says the only way to avoid being subject to A.I. is to grwo/merge with it.
<br>
<br>
Keven warwick is shoving bit of computers into the arms of his students i heard today!
<br>
<br>
(I dont know this!)
<br>
<br>
Cheers
<br>
<br>
Eldras
<br>
<br>
<br>
<br>
</p></td>
</tr>
<tr>
<td><img height="1" src="https://web.archive.org/web/20091027051655im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20091027051655im_/http://www.kurzweilai.net/mindx/images/round-bottom-left.gif"></td>
<td bgcolor="#DDDDDD" colspan="2"><img height="1" src="https://web.archive.org/web/20091027051655im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td align="right" bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20091027051655im_/http://www.kurzweilai.net/mindx/images/round-bottom-right.gif"></td>
</tr>
</table>
<a name="id42531"></a>
<table border="0" cellpadding="5" cellspacing="0" width="100%">
<tr>
<td><img height="1" src="https://web.archive.org/web/20091027051655im_/http://www.kurzweilai.net/images/blank.gif" width="40"></td>
<td colspan="4"><img height="1" src="https://web.archive.org/web/20091027051655im_/http://www.kurzweilai.net/images/blank.gif" width="639"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20091027051655im_/http://www.kurzweilai.net/mindx/images/round-left.gif"></td>
<td bgcolor="#CCCCCC"><p>Re: "Monsters of the Id" ( AI mustn't have bio-substrate)<br><span class="mindxheader"><i>posted on 07/07/2005 4:44 PM by <a href="https://web.archive.org/web/20091027051655/http://www.kurzweilai.net/mindx/profile.php?id=471">eldras</a></i></span></p></td>
<td align="right" bgcolor="#CCCCCC" valign="top"><span class="mindxheader">[<a href="#discussion">Top</a>]<br>[<a href="https://web.archive.org/web/20091027051655/http://www.kurzweilai.net/mindx/frame.html?main=show_thread.php?rootID%3D1556%23id42531" target="_top">Mind&#183;X</a>]<br>[<a href="https://web.archive.org/web/20091027051655/http://www.kurzweilai.net/mindx/frame.html?main=post.php?reply%3D42531" target="_top">Reply to this post</a>]</span></td>
<td align="right" bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20091027051655im_/http://www.kurzweilai.net/mindx/images/round-right.gif"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#DDDDDD" colspan="4"><p>drat</p></td>
</tr>
<tr>
<td><img height="1" src="https://web.archive.org/web/20091027051655im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20091027051655im_/http://www.kurzweilai.net/mindx/images/round-bottom-left.gif"></td>
<td bgcolor="#DDDDDD" colspan="2"><img height="1" src="https://web.archive.org/web/20091027051655im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td align="right" bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20091027051655im_/http://www.kurzweilai.net/mindx/images/round-bottom-right.gif"></td>
</tr>
</table>
<a name="id67293"></a>
<table border="0" cellpadding="5" cellspacing="0" width="100%">
<tr>
<td><img height="1" src="https://web.archive.org/web/20091027051655im_/http://www.kurzweilai.net/images/blank.gif" width="0"></td>
<td colspan="4"><img height="1" src="https://web.archive.org/web/20091027051655im_/http://www.kurzweilai.net/images/blank.gif" width="679"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20091027051655im_/http://www.kurzweilai.net/mindx/images/round-left.gif"></td>
<td bgcolor="#CCCCCC"><p>Re: What is Friendly AI?<br><span class="mindxheader"><i>posted on 09/12/2006 3:48 PM by <a href="https://web.archive.org/web/20091027051655/http://www.kurzweilai.net/mindx/profile.php?id=3169">mindx back-on-track</a></i></span></p></td>
<td align="right" bgcolor="#CCCCCC" valign="top"><span class="mindxheader">[<a href="#discussion">Top</a>]<br>[<a href="https://web.archive.org/web/20091027051655/http://www.kurzweilai.net/mindx/frame.html?main=show_thread.php?rootID%3D1556%23id67293" target="_top">Mind&#183;X</a>]<br>[<a href="https://web.archive.org/web/20091027051655/http://www.kurzweilai.net/mindx/frame.html?main=post.php?reply%3D67293" target="_top">Reply to this post</a>]</span></td>
<td align="right" bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20091027051655im_/http://www.kurzweilai.net/mindx/images/round-right.gif"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#DDDDDD" colspan="4"><p>back-on-track</p></td>
</tr>
<tr>
<td><img height="1" src="https://web.archive.org/web/20091027051655im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20091027051655im_/http://www.kurzweilai.net/mindx/images/round-bottom-left.gif"></td>
<td bgcolor="#DDDDDD" colspan="2"><img height="1" src="https://web.archive.org/web/20091027051655im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td copy="right" bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20091027051655im_/http://www.kurzweilai.net/mindx/images/round-bottom-right.gif"></td>
</tr>
</table>
<p></p></td>
<td>&#160;</td>
</tr>
</table>
</td></tr></table>
</body>
</html>
